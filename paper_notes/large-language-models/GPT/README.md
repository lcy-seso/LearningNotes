# Reading Lists

## LLM models

1. **GPT**: "Improving Language Understanding by Generative Pre-Training". `OpenAI(2018)` [[Project]](https://openai.com/blog/language-unsupervised/)
1. **GPT-2**: "Language Models are Unsupervised Multitask Learners". `OpenAI(2019)` [[Project]](https://openai.com/blog/better-language-models/)
1. **BERT**: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". `NAACL(2019)` [[PDF]](https://arxiv.org/pdf/1810.04805.pdf) [[Code]](https://github.com/google-research/bert)
1. **XLNet**: "XLNet: Generalized Autoregressive Pretraining for Language Understanding". `NeurIPS(2019)` [[PDF]](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf) [[Code]](https://github.com/zihangdai/xlnet)
1. **GPT-3**: "Language Models are Few-Shot Learners". `NeurIPS(2020)` [[PDF]](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) [[Code]](https://github.com/openai/gpt-3)
1. **T5**: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer". `JMLR(2020)` [[PDF]](https://jmlr.org/papers/volume21/20-074/20-074.pdf) [[Code]](https://github.com/google-research/text-to-text-transfer-transformer)
1. **InstructGPT**: "Aligning language models to follow instructions"[[blog]](https://openai.com/research/instruction-following)[[Report]](https://arxiv.org/pdf/2203.02155.pdf)
1. "**PaLM**: Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance"[[Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)[[PDF]](https://arxiv.org/pdf/2204.02311.pdf)
1. **GPT-4**: "GPT-4 Technical Report"[[PDF]](https://cdn.openai.com/papers/gpt-4.pdf)

## Adapter

1. "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning". `ICML(2019)` [[PDF]](http://proceedings.mlr.press/v97/stickland19a/stickland19a.pdf) [[Code]](https://github.com/AsaCooperStickland/Bert-n-Pals)
2. **Adapter**: "Parameter-Efficient Transfer Learning for NLP". `ICML(2019)` [[PDF]](http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf) [[Code]](https://github.com/google-research/adapter-bert)
3. **AdapterDrop**: "AdapterDrop: On the Efficiency of Adapters in Transformers". `EMNLP(2021)` [[PDF]](https://aclanthology.org/2021.emnlp-main.626.pdf)
4. "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation". `ACL(2021)` [[PDF]](https://aclanthology.org/2021.acl-long.172.pdf)
5. "Learning to Generate Task-Specific Adapters from Task Description". `ACL(2021)` [[PDF]](https://aclanthology.org/2021.acl-short.82.pdf) [[Code]](https://github.com/INK-USC/hypter)

## In-context Learning

1. **A Survey for In-context Learning**.

   *Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui*.  [[pdf](https://arxiv.org/abs/2301.00234)], 2022.12

## RLHF

1. RLHF: "Reinforcement Learning from Human Feedback"[[wiki]](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)[[a good blog]](https://huyenchip.com/2023/05/02/rlhf.html)