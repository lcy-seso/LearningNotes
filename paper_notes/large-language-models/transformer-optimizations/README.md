# Reading List

1. "Tabi: An Efficient Multi-Level Inference System for Large Language Models"[[PDF]](https://dl.acm.org/doi/pdf/10.1145/3552326.3587438)
1. "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs" [[PDF]](https://arxiv.org/abs/2210.03052) [[codes]](https://github.com/bytedance/ByteTransformer)
1. "SpecInfer: Accelerating Generative LLM Serving
with Speculative Inference and Token Tree
Verification"[[PDF]](https://arxiv.org/pdf/2305.09781.pdf)
1. "FlexGen: High-Throughput Generative Inference of Large Language Models
with a Single GPU"[[PDF]](https://arxiv.org/pdf/2303.06865.pdf)
1. "Fast Distributed Inference Serving for Large Language Models" [[PDF]](https://arxiv.org/pdf/2305.05920.pdf)
1. "ALT: Breaking the Wall between Data Layout and Loop Optimizations for Deep Learning Compilation" [[PDF]](https://dl.acm.org/doi/pdf/10.1145/3552326.3587440)

# 一些背景资料

1. [分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)
1. [On Layer Normalization in the Transformer Architecture](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf)
1. [Accelerated Inference for Large Transformer Models Using NVIDIA Triton Inference Server](https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/)
   
<p align="center">
<img src="figures/pre-post-layer-normalization-in-transformer.png" width=50%>
</p>

# 一些项目

1. [xFormer](https://github.com/facebookresearch/xformers)
1. [Faster Transformer](https://github.com/NVIDIA/FasterTransformer)
1. [mosec](https://github.com/mosecorg/mosec)