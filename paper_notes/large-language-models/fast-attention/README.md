# Reading List

1. "xFormers: A modular and hackable Transformer modelling library" [[codes]](https://github.com/facebookresearch/xformers)
1. “Simple Local Attentions Remain Competitive for Long-Context Tasks” [[PDF]](https://arxiv.org/pdf/2112.07210.pdf)
1. "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"[[PDF]](https://arxiv.org/pdf/2006.16236.pdf)
1. "Linformer: Self-Attention with Linear Complexity"[[PDF]](https://arxiv.org/abs/2006.04768)
1. "Reformer: The Efficient Transformer"[[PDF]](https://arxiv.org/abs/2001.04451)