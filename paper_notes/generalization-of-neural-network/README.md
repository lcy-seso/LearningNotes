### Numeric/experimental investigations

- Wilson D R, Martinez T R. The general inefficiency of batch training for gradient descent learning[J]. Neural Networks, 2003, 16(10): 1429-1451.
- Keskar N S, Mudigere D, Nocedal J, et al. [On large-batch training for deep learning: Generalization gap and sharp minima](rendered/On_Large-Batch_Training_for_Deep_Learning.pdf)[J]. arXiv preprint arXiv:1609.04836, 2016.

### Theoretial Analysis

> Background Knowledge: [Bayesian model comparison](rendered/Bayesian_Model_Comparison.pdf)

- Smith S L, Le Q V. [A bayesian perspective on generalization and stochastic gradient descent](rendered/A_Bayesian_Perspective_on_Generalization_and_Stochastic_Gradient_Descent.pdf)[C]//Proceedings of Second workshop on Bayesian Deep Learning (NIPS 2017). 2017.
- Hoffer E, Hubara I, Soudry D. [Train longer, generalize better: closing the generalization gap in large batch training of neural networks](rendered/Train_Longer_Generalize_Better.pdf)[C]//Advances in Neural Information Processing Systems. 2017: 1729-1739.

### Proposed Solutions

- [Large batch training of convolutional networks](rendered/Large_Batch_Training_of_Convolutional_Networks.pdf)
- Devarakonda A, Naumov M, Garland M. AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks[J]. arXiv preprint arXiv:1712.02029, 2017.
- Masters D, Luschi C. Revisiting Small Batch Training for Deep Neural Networks[J]. arXiv preprint arXiv:1804.07612, 2018.
- [Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes](rendered/Highly_Scalable_Deep_Learning_Training_System_with_Mixed-Precision.pdf)
