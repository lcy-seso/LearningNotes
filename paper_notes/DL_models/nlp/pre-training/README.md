# Reading List


1. [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)

---

1. ULM-FiT (fast.ai 2018): [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)
1. ELMo (AllenNLP, 2018, NAACL 2018 the best paper award): [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)
    - [Official blog](https://allennlp.org/elmo)
1. GPT (OpenAI, 2018): [Improving Language Understanding by Generative Pre Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    - Official blogL [Improving Language Understanding with Unsupervised Learning](https://openai.com/blog/language-unsupervised/)
1. GPT-2 (OpenAI, ): [Language Models are Unsupervised Multitask Learners]()
    - Official blog: [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)
    - [github](https://github.com/openai/gpt-2)
1. BERT (Google AI Language, 2018) [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
1. TransformerXL (Google Brain, 2019): [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)
1. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf?)
1. [MASS: Masked Sequence to Sequence Pre-training
Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/pdf/1905.02450.pdf)
1. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
1. Megatron-LM (Nvidia, 2019): [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
1. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
    - PR: [Meet ALBERT: a new ‘Lite BERT’ from Google & Toyota with State of the Art NLP performance and 18x fewer parameters](https://medium.com/@lessw/meet-albert-a-new-lite-bert-from-google-toyota-with-state-of-the-art-nlp-performance-and-18x-df8f7b58fa28)

---

1. [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf)
1. [One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling](https://arxiv.org/abs/1312.3005)
