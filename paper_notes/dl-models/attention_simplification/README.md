## Attention simplification

- [ ] [Memory-efficient Transformers via Top-$k$ Attention (arxiv.org)](https://arxiv.org/abs/2106.06899)

    21年，24 citations，[github](https://github.com/ag1988/top_k_attention)，基于PyTorch实现的一个attention变种模型；

- [ ] [Sparse Sinkhorn Attention (arxiv.org)](https://arxiv.org/abs/2002.11296)

    20年，

- [ ] [Adaptively Sparse Transformers (arxiv.org)](https://arxiv.org/abs/1909.00015)

    19年，[github](https://github.com/deep-spin/entmax)

- [ ] [TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer](https://ieeexplore.ieee.org/document/9773212)

- [x] [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

  19年，1621 citations

  https://openai.com/index/sparse-transformer/

- [x] [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
    20年，3530 citation，[github](https://github.com/allenai/longformer)

- [ ] [Star-Transformer](https://arxiv.org/abs/1902.09113)

    19年，307 citations

- [x] [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)
  
    23年，72 citations，[github](https://github.com/microsoft/torchscale/blob/main/torchscale/model/LongNet.py#L53)

- [ ] [GMAT: Global Memory Augmentation for Transformers](https://arxiv.org/abs/2006.03274)
  
   20年，47 citations

- [ ] [Efficient Streaming Language Models with Attention Sinks (arxiv.org)](https://arxiv.org/abs/2309.17453)
  
   23年，131 citations


- [ ] [Adapting Language Models to Compress Contexts](https://arxiv.org/abs/2305.14788)

  23年，52 citations


- [ ] [Landmark Attention: Random-Access Infinite Context Length for Transformers](https://arxiv.org/abs/2305.16300)

  23年，49 citations

- [ ] [Faster Causal Attention Over Large Sequences Through Sparse Flash Attention](https://arxiv.org/abs/2306.01160)

  23年，8 citations

- [ ] [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997)

  21年，505 citations

- [x] [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

  20年，2377 citations

- [ ] [Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time](https://arxiv.org/abs/2305.17118)
  
   24年，37 citations

- [ ] [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2306.14048)
  
  24年，75 citations

- [ ] [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers](https://arxiv.org/abs/2305.15805)

  24年，17 citations
  

- [ ] [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736)

    23年，56 citations

- [ ] [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://aclanthology.org/2023.emnlp-main.391/)

    23年，21 citations

- [ ] [Learning to Compress Prompts with Gist Tokens](https://arxiv.org/abs/2304.08467)

    24年，76 citations

- [ ] [Mistral 7B](https://arxiv.org/abs/2310.06825)

    23年，253 citations


[A Survey on Efficient Inference for Large Language Models](https://arxiv.org/pdf/2404.14294)