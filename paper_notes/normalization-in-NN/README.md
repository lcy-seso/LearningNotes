- [Layer Normalization](Layer_Normalization/layer_normalization.pdf)
- [Weight Normalizaton](Weight_Normalization/weight_normalization.pdf)
- A good blog to explain why batch normalization works intuitively: [An Intuitive Explanation of Why Batch Normalization Really Works]( http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/)

  >_Batch normalization makes the mean and variance of the activations of each layer independent from the values themselves. This means that the magnitude of the higher order interactions are going to be suppressed, allowing larger learning rates to be used._
