@article{thomas2018volta,
  title={\href{https://on-demand.gputechconf.com/gtc/2018/presentation/s81006-volta-architecture-and-performance-optimization.pdf}{VOLTA Architecture and performance optimization}},
  author={Thomas-Collignon, Guillaume and Micikevicius, Paulius},
  journal={GTC presentation},
  year={2018}
}
@article{linear-attention-kexue,
  title={\href{https://kexue.fm/archives/7546}{线性Attention的探索：Attention必须有个Softmax吗？}},
  author={},
  journal={科学网},
  year={2020}
}
@article{lru-kexue,
  title={\href{https://kexue.fm/archives/9554}{Google新作试图“复活”RNN：RNN能否再次辉煌？}},
  author={},
  journal={科学网},
  year={2020}
}
@article{yang2023gated,
  title={\href{https://arxiv.org/pdf/2312.06635.pdf}{Gated Linear Attention Transformers with Hardware-Efficient Training}},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2023}
}
@inproceedings{hua2022transformer,
  title={\href{https://proceedings.mlr.press/v162/hua22a/hua22a.pdf}{Transformer quality in linear time}},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={9099--9117},
  year={2022},
  organization={PMLR}
}
@inproceedings{katharopoulos2020transformers,
  title={\href{https://arxiv.org/pdf/2006.16236.pdf}{Transformers are rnns: Fast autoregressive transformers with linear attention}},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}
@article{sun2023retentive,
  title={\href{https://arxiv.org/pdf/2307.08621.pdf}{Retentive network: A successor to transformer for large language models}},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}
@article{orvieto2023resurrecting,
  title={\href{https://arxiv.org/pdf/2303.06349.pdf}{Resurrecting recurrent neural networks for long sequences}},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  journal={arXiv preprint arXiv:2303.06349},
  year={2023}
}
@article{peng2023rwkv,
  title={\href{https://arxiv.org/pdf/2305.13048.pdf}{RWKV: Reinventing RNNs for the Transformer Era}},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}
@article{gu2023mamba,
  title={\href{https://arxiv.org/pdf/2312.00752.pdf}{Mamba: Linear-time sequence modeling with selective state spaces}},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@article{gu2021efficiently,
  title={\href{https://arxiv.org/pdf/2111.00396.pdf}{Efficiently modeling long sequences with structured state spaces}},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}
@article{smith2022simplified,
  title={\href{https://arxiv.org/pdf/2208.04933.pdf}{Simplified state space layers for sequence modeling}},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal={arXiv preprint arXiv:2208.04933},
  year={2022}
}